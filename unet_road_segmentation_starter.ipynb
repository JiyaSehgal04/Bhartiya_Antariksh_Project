{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f8156d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\sehga\\miniconda3\\lib\\site-packages (0.22.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\sehga\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.7.1 in c:\\users\\sehga\\miniconda3\\lib\\site-packages (from torchvision) (2.7.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sehga\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sehga\\appdata\\roaming\\python\\python312\\site-packages (from torch==2.7.1->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sehga\\appdata\\roaming\\python\\python312\\site-packages (from torch==2.7.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\sehga\\appdata\\roaming\\python\\python312\\site-packages (from torch==2.7.1->torchvision) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\sehga\\appdata\\roaming\\python\\python312\\site-packages (from torch==2.7.1->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sehga\\appdata\\roaming\\python\\python312\\site-packages (from torch==2.7.1->torchvision) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sehga\\appdata\\roaming\\python\\python312\\site-packages (from torch==2.7.1->torchvision) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sehga\\appdata\\roaming\\python\\python312\\site-packages (from torch==2.7.1->torchvision) (75.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sehga\\appdata\\roaming\\python\\python312\\site-packages (from sympy>=1.13.3->torch==2.7.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sehga\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch==2.7.1->torchvision) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bf85246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.images = os.listdir(image_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df02cb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Basic transforms (resize + to tensor)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b30c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.images = os.listdir(image_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "881978cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc1 = self.conv_block(3, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.up = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec1 = self.conv_block(128, 64)\n",
    "        self.final = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "    def conv_block(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        d = self.up(e2)\n",
    "        d = torch.cat([d, e1], dim=1)\n",
    "        d = self.dec1(d)\n",
    "        return torch.sigmoid(self.final(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9f5c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def dice_loss(pred, target, smooth=1.):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()\n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = 1 - ((2. * intersection + smooth) /\n",
    "                (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth))\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "def train_model(model, dataloader, optimizer, criterion, device, num_epochs=5):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for images, masks in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            masks = masks.unsqueeze(1)  # add channel dimension\n",
    "\n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, masks) + dice_loss(preds, masks)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79a5a1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 'data/images' or 'data/masks' directory does not exist.\n",
      "Creating synthetic dataset for training demonstration...\n",
      "Created synthetic dataset with 20 samples\n",
      "‚úÖ Model, criterion, and optimizer created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Check if data directories exist\n",
    "if not os.path.exists(\"data/images\") or not os.path.exists(\"data/masks\"):\n",
    "    print(\"Warning: 'data/images' or 'data/masks' directory does not exist.\")\n",
    "    print(\"Creating synthetic dataset for training demonstration...\")\n",
    "    \n",
    "    # Create a synthetic dataset for demonstration\n",
    "    class SyntheticRoadDataset(Dataset):\n",
    "        def __init__(self, num_samples=20, transform=None):\n",
    "            self.num_samples = num_samples\n",
    "            self.transform = transform\n",
    "        \n",
    "        def __len__(self):\n",
    "            return self.num_samples\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            # Generate synthetic road image (random noise with some structure)\n",
    "            image = torch.randn(3, 256, 256)\n",
    "            # Generate synthetic mask (random binary mask)\n",
    "            mask = torch.randint(0, 2, (256, 256)).float()\n",
    "            \n",
    "            if self.transform:\n",
    "                # Convert to PIL for transforms compatibility\n",
    "                image_pil = transforms.ToPILImage()(image)\n",
    "                mask_pil = transforms.ToPILImage()(mask.unsqueeze(0))\n",
    "                \n",
    "                image = self.transform(image_pil)\n",
    "                mask = self.transform(mask_pil)\n",
    "            \n",
    "            return image, mask\n",
    "    \n",
    "    # Use synthetic dataset\n",
    "    dataset = SyntheticRoadDataset(num_samples=20, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "    print(f\"Created synthetic dataset with {len(dataset)} samples\")\n",
    "    \n",
    "else:\n",
    "    # Use real dataset\n",
    "    dataset = RoadDataset(\"data/images\", \"data/masks\", transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "    print(f\"Loaded real dataset with {len(dataset)} samples\")\n",
    "\n",
    "# Model, loss, optimizer (always create these)\n",
    "model = UNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"‚úÖ Model, criterion, and optimizer created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "805e60b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Dataset size: 20 samples\n",
      "Batch size: 4\n",
      "Number of batches: 5\n",
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Training failed with error: Using a target size (torch.Size([4, 1, 1, 256, 256])) that is different to the input size (torch.Size([4, 1, 256, 256])) is deprecated. Please ensure they have the same size.\n",
      "This might be due to data format issues or other training-related problems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n",
    "print(f\"Batch size: {dataloader.batch_size}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "try:\n",
    "    train_model(model, dataloader, optimizer, criterion, device, num_epochs=5)\n",
    "    print(\"üéâ Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed with error: {e}\")\n",
    "    print(\"This might be due to data format issues or other training-related problems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "824ad991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model with synthetic data...\n",
      "Test input shape: torch.Size([2, 3, 256, 256])\n",
      "Model created and moved to device: cpu\n",
      "Test output shape: torch.Size([2, 1, 256, 256])\n",
      "Output value range: [0.5241, 0.5391]\n",
      "‚úÖ Model test completed successfully!\n",
      "Test loss: 0.6949\n",
      "‚úÖ Training setup test completed successfully!\n",
      "Test output shape: torch.Size([2, 1, 256, 256])\n",
      "Output value range: [0.5241, 0.5391]\n",
      "‚úÖ Model test completed successfully!\n",
      "Test loss: 0.6949\n",
      "‚úÖ Training setup test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test the model with synthetic data since real dataset is not available\n",
    "print(\"Testing the model with synthetic data...\")\n",
    "\n",
    "# Create synthetic test data\n",
    "test_batch_size = 2\n",
    "test_images = torch.randn(test_batch_size, 3, 256, 256).to(device)\n",
    "print(f\"Test input shape: {test_images.shape}\")\n",
    "\n",
    "# Initialize model for testing\n",
    "model = UNet().to(device)\n",
    "print(f\"Model created and moved to device: {device}\")\n",
    "\n",
    "# Test model forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_output = model(test_images)\n",
    "    print(f\"Test output shape: {test_output.shape}\")\n",
    "    print(f\"Output value range: [{test_output.min().item():.4f}, {test_output.max().item():.4f}]\")\n",
    "\n",
    "print(\"‚úÖ Model test completed successfully!\")\n",
    "\n",
    "# Test training setup (without actual training)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Test loss computation\n",
    "test_target = torch.randint(0, 2, (test_batch_size, 1, 256, 256)).float().to(device)\n",
    "test_loss = criterion(test_output, test_target)\n",
    "print(f\"Test loss: {test_loss.item():.4f}\")\n",
    "\n",
    "print(\"‚úÖ Training setup test completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37601e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install geopandas shapely\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box, LineString, Point\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Check if shapefile exists\n",
    "shapefile_path = \"gis_osm_buildings_a_free_1.shp\"\n",
    "\n",
    "if not os.path.exists(shapefile_path):\n",
    "    print(f\"Warning: {shapefile_path} not found. Creating synthetic road data for demonstration...\")\n",
    "    \n",
    "    # Create synthetic road data for demonstration\n",
    "    # Define bounding box coordinates (example for Pune area)\n",
    "    minx, miny, maxx, maxy = 73.75, 18.50, 73.90, 18.60\n",
    "    \n",
    "    # Create synthetic road geometries\n",
    "    roads_data = []\n",
    "    \n",
    "    # Generate some random road lines within the bounding box\n",
    "    for i in range(10):\n",
    "        # Create random line segments within the bbox\n",
    "        x1 = random.uniform(minx, maxx)\n",
    "        y1 = random.uniform(miny, maxy)\n",
    "        x2 = random.uniform(minx, maxx)\n",
    "        y2 = random.uniform(miny, maxy)\n",
    "        \n",
    "        road_line = LineString([(x1, y1), (x2, y2)])\n",
    "        roads_data.append({'geometry': road_line, 'road_id': f'road_{i}'})\n",
    "    \n",
    "    # Create GeoDataFrame\n",
    "    roads = gpd.GeoDataFrame(roads_data, crs='EPSG:4326')\n",
    "    print(f\"Created synthetic dataset with {len(roads)} road segments\")\n",
    "    \n",
    "else:\n",
    "    # Load real roads shapefile\n",
    "    roads = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Define bounding box coordinates\n",
    "minx, miny, maxx, maxy = 73.75, 18.50, 73.90, 18.60\n",
    "\n",
    "# Define your image area as bounding box\n",
    "bbox = box(minx, miny, maxx, maxy)\n",
    "clipped_roads = roads[roads.intersects(bbox)]\n",
    "\n",
    "print(f\"Clipped roads: {len(clipped_roads)} features\")\n",
    "print(f\"Bounding box: ({minx}, {miny}, {maxx}, {maxy})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26bdb802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rasterio in c:\\users\\sehga\\miniconda3\\lib\\site-packages (1.4.3)\n",
      "Requirement already satisfied: affine in c:\\users\\sehga\\miniconda3\\lib\\site-packages (from rasterio) (2.4.0)\n",
      "Requirement already satisfied: attrs in c:\\users\\sehga\\appdata\\roaming\\python\\python312\\site-packages (from rasterio) (24.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\sehga\\appdata\\roaming\\python\\python312\\site-packages (from rasterio) (2024.12.14)\n",
      "Requirement already satisfied: click>=4.0 in c:\\users\\sehga\\appdata\\roaming\\python\\python312\\site-packages (from rasterio) (8.1.8)\n",
      "Requirement already satisfied: cligj>=0.5 in c:\\users\\sehga\\miniconda3\\lib\\site-packages (from rasterio) (0.7.2)\n",
      "Requirement already satisfied: numpy>=1.24 in c:\\users\\sehga\\appdata\\roaming\\python\\python312\\site-packages (from rasterio) (1.26.4)\n",
      "Requirement already satisfied: click-plugins in c:\\users\\sehga\\miniconda3\\lib\\site-packages (from rasterio) (1.1.1.2)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\sehga\\appdata\\roaming\\python\\python312\\site-packages (from rasterio) (3.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sehga\\appdata\\roaming\\python\\python312\\site-packages (from click>=4.0->rasterio) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Rasterized 10 road features\n",
      "‚úÖ Mask saved to data/masks/pune_tile_001.png\n",
      "Mask shape: (256, 256)\n",
      "Mask value range: [0, 1]\n",
      "Non-zero pixels: 1228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install rasterio\n",
    "\n",
    "from rasterio import features\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from affine import Affine\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # Ensure we have the required variables from previous cell\n",
    "    if 'clipped_roads' not in locals() or 'minx' not in locals():\n",
    "        print(\"Error: Required variables not found. Please run the previous cell first.\")\n",
    "        raise NameError(\"Missing required variables from previous cell\")\n",
    "    \n",
    "    # Create a blank mask (256x256)\n",
    "    height, width = 256, 256\n",
    "    transform = Affine.translation(minx, miny) * Affine.scale((maxx - minx) / width, (maxy - miny) / height)\n",
    "    \n",
    "    # Check if we have any geometries to rasterize\n",
    "    if len(clipped_roads) > 0:\n",
    "        mask = features.rasterize(\n",
    "            [(geom, 1) for geom in clipped_roads.geometry],\n",
    "            out_shape=(height, width),\n",
    "            transform=transform,\n",
    "            fill=0,\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        print(f\"Rasterized {len(clipped_roads)} road features\")\n",
    "    else:\n",
    "        print(\"No road features to rasterize, creating empty mask\")\n",
    "        mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    # Create data directory if it doesn't exist\n",
    "    os.makedirs(\"data/masks\", exist_ok=True)\n",
    "    \n",
    "    # Save mask to PNG\n",
    "    from PIL import Image\n",
    "    mask_image = Image.fromarray(mask * 255)\n",
    "    mask_path = \"data/masks/pune_tile_001.png\"\n",
    "    mask_image.save(mask_path)\n",
    "    \n",
    "    print(f\"‚úÖ Mask saved to {mask_path}\")\n",
    "    print(f\"Mask shape: {mask.shape}\")\n",
    "    print(f\"Mask value range: [{mask.min()}, {mask.max()}]\")\n",
    "    print(f\"Non-zero pixels: {np.count_nonzero(mask)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating mask: {e}\")\n",
    "    print(\"Creating a simple synthetic mask instead...\")\n",
    "    \n",
    "    # Create a simple synthetic road mask\n",
    "    height, width = 256, 256\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    # Add some simple road patterns (horizontal and vertical lines)\n",
    "    mask[100:110, :] = 1  # Horizontal road\n",
    "    mask[:, 120:130] = 1  # Vertical road\n",
    "    \n",
    "    # Create data directory and save\n",
    "    os.makedirs(\"data/masks\", exist_ok=True)\n",
    "    mask_image = Image.fromarray(mask * 255)\n",
    "    mask_path = \"data/masks/pune_tile_001.png\"\n",
    "    mask_image.save(mask_path)\n",
    "    \n",
    "    print(f\"‚úÖ Synthetic mask saved to {mask_path}\")\n",
    "    print(f\"Mask shape: {mask.shape}\")\n",
    "    print(f\"Non-zero pixels: {np.count_nonzero(mask)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa456b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c772fd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
